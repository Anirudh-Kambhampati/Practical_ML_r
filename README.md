# Practical_ML_r
Assignment: Practical ML (week 4)

The goal is to choose the method that gives a satisfactory prediction accuracy. The datasets that are required are loaded first and the training and test datasets are then loaded using read.csv() . The basic idea is resampling cases and recalculating predictions from the sample and then averaging a majority vote. This initial method is called Bootstrap aggregating or Bagging. This method has a lower variance but a similar bias to the model. So we remove the values with lower variance and the first few samples are removed. Then the accuracy if this model is predicted. Although the variance has been taken care of, the bias in the model reduces the accuracy of the model. So the accuracy of the predicted model will be pretty low. To overcome this,we introduce Random Foret Classifier. Random Forest is an extention to Bagging. The concept of Random Forest is to bootstrap the variables at each split along with bootstrapping samples before split. This increases the accuracy of the model as there are now more splits and there will be more data at each split. So the accuracy will be higher. The only problem with Random Forest is that it may overfit the model if the sample size is small. Now this predicted accuracy is compared with the accuracy of the Boosting Model and the training and test set prediction accuracies are taken from the model that gives the better output.
